{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting app-store-scraper\n",
      "  Downloading app_store_scraper-0.3.5-py3-none-any.whl (8.3 kB)\n",
      "Requirement already satisfied: pandas in /Users/qbs/anaconda3/lib/python3.10/site-packages (2.2.0)\n",
      "Collecting requests==2.23.0\n",
      "  Downloading requests-2.23.0-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.4/58.4 kB\u001b[0m \u001b[31m644.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
      "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.0/128.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting idna<3,>=2.5\n",
      "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from requests==2.23.0->app-store-scraper) (2023.11.17)\n",
      "Collecting chardet<4,>=3.0.2\n",
      "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tzdata>=2022.7 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: six>=1.5 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Installing collected packages: chardet, urllib3, idna, requests, app-store-scraper\n",
      "  Attempting uninstall: chardet\n",
      "    Found existing installation: chardet 4.0.0\n",
      "    Uninstalling chardet-4.0.0:\n",
      "      Successfully uninstalled chardet-4.0.0\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.26.18\n",
      "    Uninstalling urllib3-1.26.18:\n",
      "      Successfully uninstalled urllib3-1.26.18\n",
      "  Attempting uninstall: idna\n",
      "    Found existing installation: idna 3.4\n",
      "    Uninstalling idna-3.4:\n",
      "      Successfully uninstalled idna-3.4\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "kaggle 1.5.16 requires python-slugify, which is not installed.\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\n",
      "jupyterlab-server 2.25.2 requires requests>=2.31, but you have requests 2.23.0 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.23.0 which is incompatible.\n",
      "awscli 1.31.13 requires botocore==1.33.13, but you have botocore 1.35.23 which is incompatible.\n",
      "anaconda-client 1.11.2 requires urllib3>=1.26.4, but you have urllib3 1.25.11 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed app-store-scraper-0.3.5 chardet-3.0.4 idna-2.10 requests-2.23.0 urllib3-1.25.11\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install app-store-scraper pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: playwright in /Users/qbs/anaconda3/lib/python3.10/site-packages (1.49.1)\n",
      "Requirement already satisfied: pandas in /Users/qbs/anaconda3/lib/python3.10/site-packages (2.2.0)\n",
      "Requirement already satisfied: greenlet==3.1.1 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from playwright) (3.1.1)\n",
      "Requirement already satisfied: pyee==12.0.0 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from playwright) (12.0.0)\n",
      "Requirement already satisfied: typing-extensions in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pyee==12.0.0->playwright) (4.12.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pandas) (1.24.4)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/qbs/anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install playwright pandas\n",
    "!playwright install\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nest_asyncio in /Users/qbs/anaconda3/lib/python3.10/site-packages (1.5.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed. Data saved to 'elsa_app_reviews.csv'.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Call the async function directly\n",
    "await main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Navigating to https://apps.apple.com/us/app/elsa-speak-english-learning/id1080859478...\n",
      "Scrolling to load reviews...\n",
      "Found 0 reviews on the page.\n",
      "No reviews were scraped. Please check the page structure or selectors.\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply asyncio patch for Jupyter environments\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def scrape_ios_web_reviews(app_url, max_reviews=100):\n",
    "    \"\"\"\n",
    "    Scrapes reviews from the web-based App Store for an iOS app.\n",
    "\n",
    "    Args:\n",
    "        app_url (str): The URL of the app's review page on the web-based App Store.\n",
    "        max_reviews (int): Maximum number of reviews to scrape.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict]: List of dictionaries containing scraped reviews.\n",
    "    \"\"\"\n",
    "    reviews_data = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        print(f\"Navigating to {app_url}...\")\n",
    "        await page.goto(app_url, timeout=60000)\n",
    "\n",
    "        # Scroll to load reviews dynamically\n",
    "        print(\"Scrolling to load reviews...\")\n",
    "        for _ in range(15):  # Adjust the number of scrolls if necessary\n",
    "            await page.mouse.wheel(0, 3000)\n",
    "            await asyncio.sleep(2)\n",
    "\n",
    "        # Find all review blocks\n",
    "        review_blocks = await page.query_selector_all('div.we-customer-review')  # Update this if the structure changes\n",
    "        print(f\"Found {len(review_blocks)} reviews on the page.\")\n",
    "\n",
    "        # Parse each review block\n",
    "        for idx, review in enumerate(review_blocks[:max_reviews]):\n",
    "            try:\n",
    "                user_name = await review.query_selector(\"span.we-truncate__text\")\n",
    "                user_name = await user_name.inner_text() if user_name else \"Unknown\"\n",
    "\n",
    "                rating = await review.query_selector(\"figure > div > span\")\n",
    "                rating = len(await rating.query_selector_all(\"span[aria-hidden=true]\")) if rating else \"Unknown\"\n",
    "\n",
    "                review_text = await review.query_selector(\"blockquote\")\n",
    "                review_text = await review_text.inner_text() if review_text else \"No Review\"\n",
    "\n",
    "                date = await review.query_selector(\"time\")\n",
    "                date = await date.inner_text() if date else \"Unknown\"\n",
    "\n",
    "                reviews_data.append({\n",
    "                    \"User\": user_name,\n",
    "                    \"Rating\": rating,\n",
    "                    \"Review\": review_text,\n",
    "                    \"Date\": date\n",
    "                })\n",
    "\n",
    "                # Debug log for each review\n",
    "                print(f\"Review {idx + 1}: {user_name}, Rating: {rating}, Date: {date}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing review {idx + 1}: {e}\")\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return reviews_data\n",
    "\n",
    "async def main():\n",
    "    APP_URL = \"https://apps.apple.com/us/app/elsa-speak-english-learning/id1080859478\"  # Web-based URL for reviews\n",
    "    reviews = await scrape_ios_web_reviews(APP_URL, max_reviews=100)\n",
    "\n",
    "    if reviews:\n",
    "        # Save to CSV\n",
    "        df = pd.DataFrame(reviews)\n",
    "        df.to_csv(\"/Users/qbs/elsa_app_reviews.csv\", index=False)\n",
    "        print(\"Scraping completed. Data saved to 'elsa_app_reviews.csv'.\")\n",
    "        print(df.head())\n",
    "    else:\n",
    "        print(\"No reviews were scraped. Please check the page structure or selectors.\")\n",
    "\n",
    "# Run the script\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
